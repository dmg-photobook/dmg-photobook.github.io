<!DOCTYPE HTML>
<!--
  Phantom by HTML5 UP
  html5up.net | @ajlkn
  Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
  <head>
    <title>The PhotoBook Task and Dataset - Reference Chains</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="assets/css/main.css" />
    <link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
    <noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
  </head>
  <body class="is-preload">
    <!-- Wrapper -->
      <div id="wrapper">

        <!-- Header -->
          <header id="header">
            <div class="inner">

              <!-- Logo -->
              <a href="index.html" class="logo">
                  <span class="symbol"><img src="images/fb_logo_2.png" alt="" /></span><span class="title">The PhotoBook Task and Dataset</span>
                </a>
                

              <!-- Nav -->
                <nav>
                  <ul>
                    <li><a href="#menu">Menu</a></li>
                  </ul>
                </nav>

            </div>
          </header>

        <!-- Menu -->
          <nav id="menu">
            <h2>Menu</h2>
            <ul>
              <li><a href="index.html">Home</a></li>
              <li><a href="dataset.html">The Dataset</a></li>
              <li><a href="analysis.html">Analysis</a></li>
              <li><a href="reference_chains.html">Reference Chains</a></li>
              <li><a href="models.html">Models</a></li>
              <li><a href="team.html">Team</a></li>
              <li><a href="downloads.html">Downloads</a></li>
              <!-- <li><a href="elements.html">Elements</a></li>-->
            </ul>
          </nav>
        <!-- Main -->
          <div id="main">
            <div class="inner">
              <h1>Reference Chains</h1> 
              
              <h2>Refer, Reuse, Reduce: Grounding Subsequent References in Visual and Conversational Contexts</h2>
              <p>In PhotoBook, participants can freely interact via chat so the dialogues include different types of dialogue act. 
                In <b>Refer, Reuse, Reduce: Grounding Subsequent References in Visual and Conversational Contexts</b>, we concentrate 
                on a specific type of dialogue contribution, referring utterances, so we extract all messages that contain an image description 
                and their corresponding image target from the PhotoBook dialogues. The extraction procedure is described in our
                <a href="reference_chains.html">paper</a></li> and the corresponding code is on our GitHub <a href="reference_chains.html">repository</a></li>.
                The result of this procedure is a new dataset of reference chains made up of 41,340 referring utterances and 16,525 chains.
                You can download the dataset <a href="reference_chains.html">here</a></li>.</p>
    
              
        
<!--             <a href="#heuristics">Segmentation Heuristics</a><br />
            <a href="#sample">Sample Segment Chain</a><br />
            <a href="#evaluation">Segmentation Evaluation and Validation</a><br />
            <a href="#chain_dataset">Segment Chain Data Set</a><br />
            <br />
            <br />  -->

              
<h2>The PhotoBook Dataset: Building Common Ground through Visually Grounded Dialogue</h2>
  
<h3 id="heuristics"> Segmentation Heuristics </h3>

Collecting reference chains from dialogue data is
a non-trivial task which normally requires manual annotation. In <b>The PhotoBook Dataset: Building Common Ground through Visually Grounded Dialogue</b>, we propose
a simple procedure to automatically extract reference chains made up of dialogue segments. A dialogue segment is defined as a collection of consecutive utterances that, as a whole, discuss a given
target image and include expressions referring to
it. <b>All dialogue segments within a game that refer
to the same target image form its reference chain</b>.
In order to automatically segment the collected
dialogues in this way, <b>we developed a rule-based
heuristics exploiting participants’ image labelling
actions to detect segment boundaries and their respective targets</b>. <br /><br />

Due
to the spontaneous and unrestricted nature of the
PhotoBook dialogues, participant labelling actions do not always indicate segment boundaries
as cleanly as possible. To improve the quality of
extracted dialogue segments and reference chains,
we therefore developed a more context-sensitive
heuristics to automate segmentation. <b>The heuristics is implemented as a binary decision tree that
uses labelling actions as well as any preceding and
subsequent messages</b> and additional labelling actions to better decide on segment boundaries and
associated target images. It considers 32 combinations of eight different factors. The first case of
the heuristics, for example, states that if <br /><br />
<ol>
<li> the current turn is a message, </li>
<li> the previous turn was an image labelling action, </li>
<li> the previous turn was by the other participant,</li>
<li> the next turn is an image selection action,</li>
<li> the next turn is by the current participant,</li>
<li> the next labelling action assigns a common label,</li>
<li> the other participant’s previous labelling and
the current participant’s next labelling address
the same target image, and</li>
<li> there is a non-empty, currently developing dialogue segment,</li>
</ol>
then we assume that after one speaker selected an
image as common, the other speaker makes one
utterance and marks the same image as common,
which is resolved by saving the currently developing segment with the common image as referent
and initialising a new segment with the trailing
utterance of the second speaker. This prevents
creating a segment with just the trailing utterance
(that cannot be a complete segment) which would
be the naive decision if segmenting was based
solely on labelling actions. Other cases include
whether the next turn is a message by the other
participant followed by them labelling a second
image as different (likely to indicate that two
images were discussed and the segment should
be extended by the following message as well as
the second target image) or whether none of the
preceding and subsequent turns contains labelling
actions (indicating an ongoing dialogue segment).<br /><br />

The full heuristics can be viewed <a href="segmentation_heuristics.html">here</a>. <br /><br /> The files necessary to replicate the automatic extraction of segment chains can be downloaded from github <a href="https://github.com/dmg-photobook/photobook_dataset/tree/master/segmentation" target="_blank">here</a>, or as a zipped archive <a href="https://github.com/dmg-photobook/photobook_dataset/raw/master/segmentation.zip" target="_blank">here.</a> To execute the files, place the <code>log</code> folder into the project's <code>data</code> folder. Further documentation can be found <a href="https://github.com/dmg-photobook/photobook_dataset/tree/master/segmentation" target="_blank">here</a>. <br /> <br /> 

<h3 id="sample">Sample Segment Chain</h3>

The following shows a typical example of an automatically extracted chain of dialogue segments
associated with the following image: <br /><br />

<img src="images/coco_sample.jpg" alt="" /><br /><br />

<blockquote>
<b>B</b>: Hello <br />
<b>A</b>: Hi <br />
<b>A</b>: Do you have a woman with a black 
coat with buttons, glasses and a piece 
of pizza on table <br />
<b>B</b>: no <br />
<hr>
<b>A</b>: Lady with black shirt, glasses with pizza on table? <br />
<b>B</b>: yes <br />

<b>A</b>: Table with orange bowl with lemons and
liquor, cups? <br />
<b>B</b>: no <br />
<hr>
<b>A</b>: Orange bowl with lemons, liquor? <br />
<b>B</b>: lady pizza <br />
<b>A</b>: No lady pizza <br />
<b>B</b>: yes <br />
<hr>
<b>B</b>: woman and pizza <br />
<b>A</b>: Empty kitchen wood coloured cabinets? <br />
<b>A</b>: No woman pizza <br />
<b>B</b>: no <br />
</blockquote>

<h3 id="evaluation">Segmentation Evaluation and Validation</h3>

72% of the
extracted segments are linked to only one target;
25% to two. Moreover, reference chains do not
necessarily contain one segment for each of the
five game rounds. They may contain fewer or
more segments than rounds in a game, since participants may discuss the same image more than
once in a single round and some of the extracted
chains may be noisy, as explained in the evaluation
section below. 75% of the automatically extracted
chains contain three to six segments. <br /><br />


To evaluate the segmentation, two
annotators independently reviewed segments extracted from 20 dialogues. These segments were
annotated by marking all utterances u in a segment
S with target images I that refer to an image i<sub>0</sub>
where i<sub>0</sub> ∈ I to determine precision, and marking
all directly preceding and succeeding utterances u<sub>0</sub>
outside of a segment S that refer to a target image
i ∈ I to determine recall. Additionally, if a segment S did not include any references to any of its
target images I, it was labelled as improper. 95%
of annotated segments were assessed to be proper
(Cohen’s κ of 0.87), with 28.4% of segments containing non-target references besides target references (Cohen’s κ of 0.97). Recall across all reviewed segments is 99% (Cohen’s κ of 0.93). <br/><br/>

<h3 id="chain_dataset">Segment Chain Data Set</h3>

The automatically extracted co-reference chains
per target image were split into three disjoint sets
for training (70%), validation (15%) and testing
(15%), aiming at an equal distribution of target image domains in all three sets. The raw numbers per
data split are shown in the following table: <br /><br />
<div class="table-wrapper">
  <table class="alt">
    <thead>
      <tr>
        <th>Split</th>
        <th>Chains</th>
        <th>Segments</th>
        <th>Targets</th>
        <th>Non-Targets</th>
      </tr>
    </thead>
    <tbody>
      <tr>   
<td>Train</td>
<td>12,694 </td>
<td>30,992</td>
<td>40,898</td>
<td>226,993</td>
</tr>
<tr>
<td>Val </td>
<td>2,811</td>
<td>6,801</td>
<td>9,070 </td>
<td>50,383</td>
</tr>
<tr>
<td>Test</td>
<td>2,816</td>
<td>6,876</td>
<td>9,025</td>
<td>49,774</td>
</tr>
</tbody>
  </table>
</div>

The segment chain dataset creator can be downloaded from github <a href="https://github.com/dmg-photobook/photobook_dataset/tree/master/segmentation" target="_blank">here</a>, or as a zipped archive <a href="https://github.com/dmg-photobook/photobook_dataset/raw/master/segmentation.zip" target="_blank">here</a>. To execute the files, place the <code>log</code> folder into the project's <code>data</code> folder. Further documentation can be found <a href="https://github.com/dmg-photobook/photobook_dataset/tree/master/segmentation" target="_blank">here</a>. <br /> <br />  

The produced output files can be found <a href="https://github.com/dmg-photobook/photobook_dataset/raw/master/reference_chain_data.zip" target="_blank">here</a>.

</div>
          </div>

        <!-- Footer -->
          <footer id="footer">
            <div class="inner">
              <section>
                <h2>Contact</h2>
                <ul class="icons">
                  <li> For questions and remarks about this website or the PhotoBook Task and Dataset, please contact <a href="mailto:j.haber@qmul.ac.uk"> Janosch Haber</li>
                  <li> <a href="https://github.com/dmg-photobook/photobook_dataset.git" class="icon style2 fa-github"><span class="label">GitHub</span></a></li>  
                </ul>
                Raquel Fernández<br />
                  Science Park 107, office F1.07 <br />
                  +31 (0)20 525 7009  <br />
              </section>
              <section>
                

              </section>
              <ul class="copyright">
                <li>&copy; Dialogue Modeling Group, University of Amsterdaom. All rights reserved</li><li>Design: <a href="http://html5up.net" target="_blank">HTML5 UP</a></li>
              </ul>
            </div>
          </footer>

      </div>

    <!-- Scripts -->
      <script src="assets/js/jquery.min.js"></script>
      <script src="assets/js/browser.min.js"></script>
      <script src="assets/js/breakpoints.min.js"></script>
      <script src="assets/js/util.js"></script>
      <script src="assets/js/main.js"></script>

  </body>
</html>
